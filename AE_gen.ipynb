{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad4b5c-40c2-49ae-9334-65a87687455f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import all the required libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import json\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, set_start_method\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import os\n",
    "import random\n",
    "import signal\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import copy\n",
    "from joblib import dump, load\n",
    "import gc\n",
    "import networkx as nx\n",
    "import re\n",
    "import numpy as np\n",
    "import angr, csv, sys, statistics\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001e56a-1076-4d38-9896-ff5e9c260ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a logger for angr\n",
    "logger = logging.getLogger('angr')\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "logger.propagate = False\n",
    "\n",
    "vex_logger = logging.getLogger('angr.analyses.propagator.engine_vex.SimEnginePropagatorVEX')\n",
    "vex_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "class IgnoreSpecificErrorFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return \"SimEnginePropagatorVEX | Unsupported\" not in record.getMessage()\n",
    "\n",
    "\n",
    "class IgnoreWarningsFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return record.levelno < logging.WARNING\n",
    "\n",
    "# Add the filters to the logger\n",
    "logger.addFilter(IgnoreSpecificErrorFilter())\n",
    "logger.addFilter(IgnoreWarningsFilter())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88025931-75c6-4a80-bd33-ded695a73b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bytes = [10, 25, 7, 8] # payload size(bytes)\n",
    "blockSize = 125\n",
    "\n",
    "trans = ['mov', 'movabs', 'push', 'pop', 'lb', 'lbu', 'lh', 'lw', 'sb', 'sh', 'sw', 'ldc', 'lea', 'restore', 'lswi', 'sts', 'usp', 'srs', 'pea', 'lui', 'lhu', 'rdcycle', 'rdtime', 'rdinstret']\n",
    "cal = ['add', 'sub', 'inc', 'xor', 'sar', 'addi', 'addiu', 'addu', 'and', 'ldr', 'andi', 'nor', 'or', 'ori', 'subu', 'xori', 'div', 'divu', 'mfhi', 'mflo', 'mthi', 'mtlo', 'mult', 'multu', 'sll', 'sllv', 'sra', 'srav', 'srl', 'srlv', 'bic', 'xnor', 'not', 'eor', 'asr', 'fabs', 'abs', 'mac', 'neg', 'cmp', 'test', 'slti', 'slt', 'sltu', 'sltui', 'sltiu', 'cmn', 'fcmp', 'dcbi', 'tas', 'btst', 'cbw', 'cwde', 'cdqe', 'cdq', 'slli', 'srli', 'srai', 'auipc', 'adc', 'sbb']\n",
    "ctl = ['jmp', 'jz', 'jnz', 'jne', 'je', 'call', 'jr', 'beq', 'bge', 'bgeu', 'bgez', 'bgezal', 'bgtz', 'blez', 'blt', 'bltu', 'bltz', 'bltzal', 'bne', 'break', 'j', 'jal', 'jalr', 'mfc0', 'mtc0', 'syscall', 'leave', 'hvc', 'svc', 'hlt', 'arpl', 'sys', 'ti', 'trap', 'ret', 'retn', 'bl', 'bicc', 'bclr', 'bsrf', 'rte', 'wait', 'fwait', 'wfe', 'ecall', 'ebreak', 'jb', 'jbe']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f0b40-0a4b-4c2c-8d1e-1ef6ebcefdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_feature(file_name):\n",
    "    p = angr.Project(file_name, load_options={'auto_load_libs': False})\n",
    "    # Generate a static CFG\n",
    "    cfg = p.analyses.CFGFast()\n",
    "\n",
    "    # Generate a dynamic CFG\n",
    "    #cfg = p.analyses.CFGEmulated(keep_state=True) # involves execution\n",
    "    G = cfg.graph\n",
    "    G_undirected = G.to_undirected()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # nodes & edges\n",
    "    nodes = G.number_of_nodes()\n",
    "    edges = G.number_of_edges()\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    # degree\n",
    "    idegree = {d[0]:d[1] for d in G.in_degree()}\n",
    "    odegree = {d[0]:d[1] for d in G.out_degree()}\n",
    "\n",
    "#         # normalized\n",
    "    norm_in_degree = {_:idegree[_]/sum(idegree.values()) for _ in idegree}\n",
    "    norm_out_degree = {_:odegree[_]/sum(odegree.values()) for _ in odegree}\n",
    "\n",
    "#         # mean\n",
    "    in_degree = np.mean([_ for _ in norm_in_degree.values()])\n",
    "    out_degree = np.mean([_ for _ in norm_out_degree.values()])\n",
    "\n",
    "\n",
    "    # density\n",
    "    density = nx.density(G)\n",
    "\n",
    "    # closeness_centrality\n",
    "    closeness_centrality = np.mean(list(nx.closeness_centrality(G).values()))\n",
    "\n",
    "    # betweeness_centrality\n",
    "    betweeness_centrality = np.mean(list(nx.betweenness_centrality(G).values()))\n",
    "    # connected_components\n",
    "    connected_components = nx.number_connected_components(G_undirected)\n",
    "\n",
    "#         ---------------------------------------------------------\n",
    "\n",
    "    # shortest_path\n",
    "    short_path = dict(nx.all_pairs_shortest_path(G))\n",
    "    short_path_value = {}\n",
    "\n",
    "    for i in short_path:\n",
    "        temp = {}\n",
    "        for j in short_path[i]:\n",
    "            if i != j:\n",
    "                temp[j] = len(short_path[i][j])\n",
    "        short_path_value[i] = temp\n",
    "\n",
    "#         ---------------------------------------------------------\n",
    "#         diameter and radius\n",
    "    sp = []\n",
    "    for _ in short_path_value:\n",
    "        sp.extend([i for i in short_path_value[_].values()])\n",
    "\n",
    "    diameter = max(sp)\n",
    "    radius = min(sp)\n",
    "#         ---------------------------------------------------------\n",
    "# collecting opocde features from instructions\n",
    "\n",
    "    instr_list = {'trans': 0, 'cal': 0, 'ctl': 0}\n",
    "\n",
    "    regex = re.compile('\\t\\w+\\t')\n",
    "    instruction = []\n",
    "    block_num = nodes\n",
    "    func_size = []\n",
    "\n",
    "    for n in G.nodes(data=True):\n",
    "        try:\n",
    "            block_split = p.factory.block(n[0].function_address).capstone.insns\n",
    "            func_size.append(len(block_split))\n",
    "            for __ in block_split:\n",
    "                instruction.append(regex.findall(str(__))[0][1:-1])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            #print(e)\n",
    "\n",
    "\n",
    "    for _ in instruction:\n",
    "        if _ in trans:\n",
    "            instr_list['trans'] += 1\n",
    "        elif _ in cal:\n",
    "            instr_list['cal'] += 1\n",
    "        elif _ in ctl:\n",
    "            instr_list['ctl'] += 1\n",
    "\n",
    "    # total instruction count\n",
    "    total_trans = instr_list['trans']\n",
    "    total_cal = instr_list['cal']\n",
    "    total_ctl = instr_list['ctl']\n",
    "\n",
    "    # Avg. instruction count\n",
    "    for _ in instr_list.keys():\n",
    "        instr_list[_] /= block_num\n",
    "    avg_trans = instr_list['trans']\n",
    "    avg_cal = instr_list['cal']\n",
    "    avg_ctl = instr_list['ctl']\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    avg_block = edges / block_num\n",
    "    avg_block_size = statistics.mean(func_size)\n",
    "\n",
    "    # ---------------------------------------------------------      \n",
    "\n",
    "    return [nodes, edges, out_degree, in_degree, density, closeness_centrality, betweeness_centrality, connected_components, diameter, radius, total_trans, total_cal, total_ctl, avg_trans, avg_cal, avg_ctl, avg_block, avg_block_size]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e22f61-fa6b-4046-9e41-2df64de49436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload injection into the binary\n",
    "\n",
    "def check_avail_bytes(file):\n",
    "    os.system(f'cp {file} ./test/{file.split(os.sep)[-1]}.test')\n",
    "    result = os.popen(f'./elfinjector/build/elfinjector ./test/{file.split(os.sep)[-1]}.test ./elfinjector/build/payload_a').read()\n",
    "    os.system(f'rm ./test/{file.split(os.sep)[-1]}.test')\n",
    "    msg = re.findall('\\(.*?available\\)', result)\n",
    "    if msg == []:\n",
    "        return -1\n",
    "        \n",
    "    available_bytes = int(msg[0].split()[0][1:])\n",
    "    \n",
    "    del result, msg\n",
    "    gc.collect()\n",
    "    return available_bytes    \n",
    "    \n",
    "def extend(fn):\n",
    "    os.system(f'./tpi {fn}')\n",
    "    tmp = check_avail_bytes(fn)\n",
    "    if tmp == -1:\n",
    "        return -1\n",
    "    else:\n",
    "        print(\"file name:\", (fn.split(os.sep)[-1]), \"\\nAvailable bytes:\", tmp)\n",
    "        return 0\n",
    "    \n",
    "def add_payload(fn, pn, N):\n",
    "    ab = check_avail_bytes(fn)\n",
    "    if ab == -1:\n",
    "        return -1\n",
    "    if ab <= (p_bytes[(ord(pn)-ord('a'))]*N):\n",
    "        res = extend(fn)\n",
    "    # test add payload\n",
    "    for i in range(N):\n",
    "        os.system(f'./elfinjector/build/elfinjector {fn} ./elfinjector/build/payload_{pn}')\n",
    "        \n",
    "\n",
    "def gen_AE(fn, pn, N): # file_name, payload_name, payload_num\n",
    "    if pn == 0:\n",
    "        add_payload(fn, 'a', N)\n",
    "    elif pn == 1:\n",
    "        add_payload(fn, 'b', N)\n",
    "    elif pn == 2:\n",
    "        add_payload(fn, 'c', N)\n",
    "    elif pn == 3:\n",
    "        add_payload(fn, 'd', N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5397c3-2822-46a8-9523-b0877d32ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial examples generation\n",
    "\n",
    "class Attack:\n",
    "    def __init__(self, file, model, scaler_path):\n",
    "        self.file_name = file\n",
    "        self.model = model\n",
    "        self.scaler = load(scaler_path)  \n",
    "\n",
    "    def attack(self):\n",
    "        ori_feature = extract_feature(self.file_name)\n",
    "        scaled_feature = self.scaler.transform(pd.DataFrame([ori_feature]))  \n",
    "        ori_proba = self.model.predict_proba(scaled_feature)[0][0]  # benign probability\n",
    "        print(\"Original Probability: \", ori_proba)\n",
    "        attack_seq = []\n",
    "\n",
    "        proba = ori_proba\n",
    "        feature = ori_feature\n",
    "        \n",
    "        max_scale = 150  # maximum stagnation allowed\n",
    "        scale = 1\n",
    "        T = 0 \n",
    "        flag = 0\n",
    "        Thd = 1000 \n",
    "        while proba < 0.5: \n",
    "            max_proba = proba \n",
    "            max_seq = copy.deepcopy(attack_seq)\n",
    "            mi = check_payload_num(max_seq)  \n",
    "\n",
    "            # ------------------------------------------------------------------------------------------------\n",
    "            \n",
    "            for i in range(4): \n",
    "                if i != mi: \n",
    "                    os.system(f'cp {self.file_name} {self.file_name}.bak') \n",
    "                    r = extend(f'{self.file_name}.bak') \n",
    "                    if attack_seq != []: \n",
    "                        for j in attack_seq: \n",
    "                            gen_AE(f'{self.file_name}.bak', j[0], j[1]) \n",
    "    \n",
    "                    test_seq = copy.deepcopy(attack_seq)\n",
    "                    test_seq.append((i, (int(blockSize/p_bytes[i]) * scale))) \n",
    "    \n",
    "                    gen_AE(f'{self.file_name}.bak', test_seq[-1][0], test_seq[-1][1])\n",
    "    \n",
    "                    feature = extract_feature(f'{self.file_name}.bak')\n",
    "                    scaled_feature = self.scaler.transform(pd.DataFrame([feature]))  # Scale the feature\n",
    "                    os.system(f'rm {self.file_name}.bak')\n",
    "    \n",
    "                    proba_a = self.model.predict_proba(scaled_feature)[0][0]\n",
    "    \n",
    "                    if proba_a > (get_thd(max_proba, Thd)):\n",
    "                        max_proba = proba_a\n",
    "                        max_seq = copy.deepcopy(test_seq)\n",
    "                        print(\"Max sequence\", max_seq, \"Max proba\", max_proba )\n",
    "\n",
    "                    gc.collect()\n",
    "            # -----------------------------------------------------------------------------------------------------------------\n",
    "            T += 1\n",
    "            if Thd > 0: \n",
    "                Thd -= 1 \n",
    "            if max_seq == attack_seq: \n",
    "                if scale >= max_scale: \n",
    "                    break\n",
    "                flag += 1 \n",
    "                scale += flag \n",
    "                continue\n",
    "            else:            \n",
    "                flag = 0\n",
    "                scale = 1    \n",
    "                attack_seq = copy.deepcopy(max_seq)\n",
    "                proba = max_proba\n",
    "\n",
    "\n",
    "        #for j in attack_seq:\n",
    "        #    os.system(f'cp {self.file_name} ./adv_samples{self.file_name}') \n",
    "        #    r = extend(f'./adv_samples{self.file_name}') \n",
    "        #    gen_AE(f'./adv_samples{self.file_name}', j[0], j[1]) \n",
    "                \n",
    "        if proba < 0.5:\n",
    "            label = 1 \n",
    "            attack_num = [0, 0, 0, 0]\n",
    "            used_bytes = 0\n",
    "            seq = \"\" \n",
    "            for i in attack_seq:\n",
    "                used_bytes += i[1] * p_bytes[i[0]]\n",
    "                attack_num[i[0]] += i[1]\n",
    "                seq += str(i[0])\n",
    "        else:\n",
    "            label = 0  # benign \n",
    "            attack_num = [0, 0, 0, 0]\n",
    "            used_bytes = 0\n",
    "            seq = \"\" \n",
    "            for i in attack_seq:\n",
    "                used_bytes += i[1] * p_bytes[i[0]]\n",
    "                attack_num[i[0]] += i[1]\n",
    "                seq += str(i[0])\n",
    "\n",
    "        return {'label': label, 'used_bytes': used_bytes, 'attack_num': attack_num, 'iter': T, 'attack_seq': seq}\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "def get_thd(x, Thd):\n",
    "    return x + np.random.uniform(low=-0.00002, high=0.00005) * Thd   # adjust the values\n",
    "\n",
    "# avoid overusing one payload\n",
    "def check_payload_num(seq):\n",
    "    if seq == []:\n",
    "        return 100\n",
    "    pn = [0 for i in range(4)]\n",
    "    for i in seq:\n",
    "        if i[0] == 0:\n",
    "            pn[0] += i[1]\n",
    "        elif i[0] == 1:\n",
    "            pn[1] += i[1]\n",
    "        elif i[0] == 2:\n",
    "            pn[2] += i[1]\n",
    "        elif i[0] == 3:\n",
    "            pn[3] += i[1]\n",
    "    for i in pn:\n",
    "        if (i/sum(pn)) > 0.5:\n",
    "            return pn.index(i)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2105d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DatabaseFactory:\n",
    "\n",
    "    def __init__(self, train_db, db_name, root_path, scaler_path):\n",
    "        self.train_db = train_db\n",
    "        self.db_name = db_name\n",
    "        self.root_path = root_path\n",
    "        self.scaler_path = scaler_path\n",
    "        self.model = self.load_model()\n",
    "\n",
    "    @staticmethod\n",
    "    def worker(item):\n",
    "        DatabaseFactory.analyze_file(item)\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def insert_in_db(db_name, pool_sem, result, filename):\n",
    "        path = filename.split(os.sep)\n",
    "        pool_sem.acquire()\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute('''INSERT INTO functions VALUES (?,?,?,?,?,?,?,?,?,?,?)''', (None,            # id\n",
    "                                                                 path[-1],     # file_name\n",
    "                                                                 result['label'],\n",
    "                                                                 result['avail_bytes'],\n",
    "                                                                 result['used_bytes'],\n",
    "                                                                 result['attack_num'][0],\n",
    "                                                                 result['attack_num'][1],\n",
    "                                                                 result['attack_num'][2],\n",
    "                                                                 result['attack_num'][3],\n",
    "                                                                 result['iter'],\n",
    "                                                                 result['attack_seq']\n",
    "                                                                 ))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        pool_sem.release()\n",
    "        \n",
    "    @staticmethod\n",
    "    def analyze_file(item):\n",
    "        global pool_sem\n",
    "        os.setpgrp()\n",
    "        \n",
    "        filename = item[0]\n",
    "        db = item[1]\n",
    "        model = item[2]\n",
    "        scaler_path = item[3]\n",
    "        \n",
    "        avail_bytes = check_avail_bytes(filename)\n",
    "        if avail_bytes == -1:\n",
    "            return -1\n",
    "        \n",
    "        analyzer = Attack(filename, model, scaler_path)\n",
    "        pool = ThreadPool(1)\n",
    "        res = pool.apply_async(analyzer.attack)\n",
    "        \n",
    "        result = res.get(9000) \n",
    "        \n",
    "        result['file_size'] = os.path.getsize(filename)\n",
    "        result['avail_bytes'] = avail_bytes\n",
    "        \n",
    "        DatabaseFactory.insert_in_db(db, pool_sem, result, filename)\n",
    "\n",
    "        analyzer.close()\n",
    "        del result, analyzer, res\n",
    "        gc.collect()\n",
    "        return 0\n",
    "    \n",
    "    def load_model(self):\n",
    "        model = load('./detector/models/RF.joblib')  # Load the model\n",
    "        return model\n",
    "\n",
    "    def create_db(self):\n",
    "        print('Database creation...')\n",
    "        conn = sqlite3.connect(self.db_name)\n",
    "        conn.execute(''' CREATE TABLE  IF NOT EXISTS functions (id INTEGER PRIMARY KEY, \n",
    "                                                                file_name text,\n",
    "                                                                label numeric,\n",
    "                                                                avail_bytes numeric,\n",
    "                                                                used_bytes numeric,\n",
    "                                                                p_loop numeric,\n",
    "                                                                p_block numeric,\n",
    "                                                                p_trans numeric,\n",
    "                                                                p_arith numeric, \n",
    "                                                                T numeric,\n",
    "                                                                attack_seq text\n",
    "                                                                );''')\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def scan_for_file(self, start):\n",
    "        file_list = []\n",
    "        directories = os.listdir(start)\n",
    "        for item in directories:\n",
    "            item = os.path.join(start, item)\n",
    "            if os.path.isdir(item):\n",
    "                file_list.extend(self.scan_for_file(item + os.sep))\n",
    "            elif os.path.isfile(item):\n",
    "                file_list.append(item)\n",
    "        return file_list\n",
    "\n",
    "    def remove_override(self, file_list):\n",
    "        conn = sqlite3.connect(self.train_db) \n",
    "        cur = conn.cursor()\n",
    "        q = cur.execute('''SELECT filename FROM functions''') \n",
    "        names = q.fetchall()\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_name)\n",
    "        cur = conn.cursor()\n",
    "        q = cur.execute('''SELECT file_name FROM functions''')\n",
    "        names.extend(q.fetchall())\n",
    "        cleaned_file_list = []\n",
    "        names = [_[0] for _ in names]\n",
    "        for f in file_list:\n",
    "            if not(f.split(os.sep)[-1] in names):\n",
    "                cleaned_file_list.append(f)\n",
    "\n",
    "        return cleaned_file_list\n",
    "\n",
    "    def build_db(self):\n",
    "        global pool_sem\n",
    "\n",
    "        pool_sem = multiprocessing.BoundedSemaphore(value=1)\n",
    "\n",
    "        self.create_db()\n",
    "        file_list = self.scan_for_file(self.root_path)\n",
    "        print('Found ' + str(len(file_list)) + ' during the scan')\n",
    "        file_list = self.remove_override(file_list)\n",
    "        print('Find ' + str(len(file_list)) + ' files to analyze')\n",
    "        random.shuffle(file_list)\n",
    "\n",
    "        t_args = [(f, self.db_name, self.model, self.scaler_path) for f in file_list]\n",
    "\n",
    "        p = Pool(processes=4, maxtasksperchild=4)\n",
    "        for _ in tqdm(p.imap_unordered(DatabaseFactory.worker, t_args), total=len(file_list)):\n",
    "            pass\n",
    "\n",
    "        p.close()\n",
    "        p.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcab458-bc6d-4908-ac62-7469f2b205d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    scaler_path = \"./detector/models/scaler.joblib\"\n",
    "    train_db = './train.db' \n",
    "    db = './results.db' \n",
    "    file_dir = './mal_samples/' \n",
    "    factory = DatabaseFactory(train_db, db, file_dir,scaler_path)\n",
    "    factory.build_db()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
